{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb Data Analysis: \n",
    "## Making your house the best at the platform!\n",
    "---\n",
    "This notebook is part of the Udacity's Data Scientist Nanodegree program\n",
    "\n",
    "In this project, I will be investigating Airbnb data and answering relevant questions using the **CRISP-DM** process:\n",
    "\n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Data Preparation \n",
    "4. Data Modeling\n",
    "5. Results & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Business Understanding\n",
    "\n",
    "## The 3 questions I'm looking to answer are:\n",
    "\n",
    "1. **Which city has the best listings? Which one has more expensive ones? Is there a connection in that?**\n",
    "2. **What's the connection between price and occupation rate? Where does the quality of the listing fits in that?** \n",
    "3. **What are the main features that influences the review rates? What about the prices?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reading Data\n",
    "### 2.2.1 Boston Airbnb Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading boston data\n",
    "df_bos_lis = pd.read_csv('BostonData/listings.csv')\n",
    "df_bos_rev = pd.read_csv('BostonData/reviews.csv')\n",
    "df_bos_cal = pd.read_csv('BostonData/calendar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Seattle Airbnb Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading seattle data\n",
    "df_sea_lis = pd.read_csv('SeattleData/listings.csv')\n",
    "df_sea_rev = pd.read_csv('SeattleData/reviews.csv')\n",
    "df_sea_cal = pd.read_csv('SeattleData/calendar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 First View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(df_bos_cal.head(), df_sea_cal.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(df_bos_lis.head(1), df_sea_lis.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(df_bos_rev.head(), df_sea_rev.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Reviews Dataset\n",
    "My objective with this dataset is to classify each review as good or bad. Then, I'll count how many good and bad reviews each listing has. I believe this will be a good parameter for predicting both price and review rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking first rows of the dataset\n",
    "df_bos_rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making a copy of the original dataset\n",
    "df_bos_rev_c = df_bos_rev.copy()\n",
    "\n",
    "# checking the dataset info\n",
    "df_bos_rev_c.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Reviews classification process\n",
    "1. I'll will create two lists, one with positive words and other with negative words. \n",
    "2. I'll count how many words of each list are present in each comment\n",
    "3. The category wich has more words, will be the category of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good words list\n",
    "positive_words = ['good', 'great', 'amazing', 'perfect', 'nice', 'cool', 'cozy', 'amazing', 'comfortable', 'loved',\n",
    "                  'enjoyed', 'lovely', 'wonderful', 'fantastic', 'pleasure', 'brilliant', 'pleasant', 'superb',\n",
    "                  'charming', 'awesome', 'beautiful', 'fun', 'excellent']\n",
    "\n",
    "# bad words list\n",
    "negative_words = ['bad', 'terrible', 'horrible', 'uncomfortable', 'dirty', 'cancelled', 'unconvenient', 'never',\n",
    "                  'hated', 'disliked', 'ugly', 'boring', 'refund', 'exhausted', 'tired','garbage','not acceptable',\n",
    "                  'refund', 'awful', 'damages', 'never', 'frustrated', 'canceled', 'cancel', 'panic', 'horror',\n",
    "                  'worst']\n",
    "\n",
    "# classification function\n",
    "def classify_good_or_bad(comment):\n",
    "    # making sure comment is string\n",
    "    comment = str(comment)\n",
    "    \n",
    "    # removing all sorts of punctuation\n",
    "    for char in string.punctuation:\n",
    "        comment.replace(char, '')\n",
    "    \n",
    "    # make it all lowercase\n",
    "    comment = comment.lower()\n",
    "    \n",
    "    # get all words\n",
    "    comment = comment.split()\n",
    "    \n",
    "    # good words count\n",
    "    positive_count = 0\n",
    "    for word in comment:\n",
    "        if word in positive_words:\n",
    "            positive_count += 1\n",
    "            \n",
    "    # bad words count\n",
    "    negative_count = 0\n",
    "    for word in comment:\n",
    "        if word in negative_words:\n",
    "            negative_count += 1\n",
    "            \n",
    "    # classifying \n",
    "    if positive_count > negative_count:\n",
    "        return 'positive'\n",
    "    elif positive_count < negative_count:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'unknwon'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Creating new good or bad review column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# applying the classification function\n",
    "df_bos_rev_c['review_cat'] = df_bos_rev_c.comments.apply(classify_good_or_bad)\n",
    "df_bos_rev_c['review_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Dropping unnecessary columns and creating dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pulling only the necessary columns\n",
    "df_bos_rev_c = df_bos_rev_c[['listing_id', 'review_cat']]\n",
    "\n",
    "# creating dummy variables for the good, bad or unknown reviews \n",
    "df_bos_rev_c = pd.concat([df_bos_rev_c['listing_id'], pd.get_dummies(df_bos_rev_c['review_cat'], prefix='review')], axis=1)\n",
    "df_bos_rev_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Creating the boston reviews dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating the boston reviews dataframe by grouping the amount of positive rows by listing_id\n",
    "boston_reviews = df_bos_rev_c.groupby(['listing_id']).sum().reset_index()\n",
    "boston_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This looks great! Now let's apply the same steps in the Seattle dataframe and merge them!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Applying the same steps on the Seattle dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# copying the dataframe\n",
    "df_sea_rev_c = df_sea_rev.copy()\n",
    "\n",
    "# applying the classification function\n",
    "df_sea_rev_c['review_cat'] = df_sea_rev_c.comments.apply(classify_good_or_bad)\n",
    "\n",
    "# pulling only the necessary columns\n",
    "df_sea_rev_c = df_sea_rev_c[['listing_id', 'review_cat']]\n",
    "\n",
    "# creating dummy variables for the good, bad or unknown reviews \n",
    "df_sea_rev_c = pd.concat([df_sea_rev_c['listing_id'], pd.get_dummies(df_sea_rev_c['review_cat'], prefix='review')], axis=1)\n",
    "\n",
    "# creating the boston reviews dataframe by grouping the amount of positive rows by listing_id\n",
    "seattle_reviews = df_sea_rev_c.groupby(['listing_id']).sum().reset_index()\n",
    "\n",
    "# checking if everything went ok\n",
    "seattle_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 Combining Seattle and Boston dataframes into the reviews dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews = pd.concat([boston_reviews, seattle_reviews], axis=0)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Calendar Dataset\n",
    "My objective with this dataset is to extract the percentage of the year that each listing was occupied. That way I will be able to explore how the listings features make them more or less attractive to travelers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating a copy of the dataset\n",
    "df_bos_cal_c = df_bos_cal.copy()\n",
    "\n",
    "# checking dataset info\n",
    "df_bos_cal_c.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Pulling only necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pulling only the listing and availble columns \n",
    "df_bos_cal_c = df_bos_cal_c[['listing_id', 'available']]\n",
    "df_bos_cal_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Formatting the 'available' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bos_cal_c.available.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying lambda function to make true of false\n",
    "df_bos_cal_c.available = df_bos_cal_c.available.apply(lambda x: x == 't')\n",
    "df_bos_cal_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bos_cal_c.available.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Getting the Dummies for the 'available' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_bos_cal_c = pd.concat([df_bos_cal_c.listing_id, pd.get_dummies(df_bos_cal_c.available, prefix='available')], axis=1)\n",
    "df_bos_cal_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Creating the Boston Calendar dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grouping the amount of occupied and taken days for each listing\n",
    "boston_calendar = df_bos_cal_c.groupby('listing_id').sum().reset_index()\n",
    "boston_calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating the occupation_rate column: represents the % of the year that the listing is occupied\n",
    "boston_calendar['occupation_rate'] = boston_calendar['available_False'] / 365\n",
    "\n",
    "# removing unnecessary columns\n",
    "boston_calendar = boston_calendar[['listing_id', 'occupation_rate']]\n",
    "boston_calendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks **amazing**! Now let's do the same with the seattle dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Applying the same steps on the seattle dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating a copy of the dataset\n",
    "df_sea_cal_c = df_sea_cal.copy()\n",
    "\n",
    "# pulling only the listing and availble columns \n",
    "df_sea_cal_c = df_sea_cal_c[['listing_id', 'available']]\n",
    "\n",
    "# applying lambda function to make true of false\n",
    "df_sea_cal_c.available = df_sea_cal_c.available.apply(lambda x: x == 't')\n",
    "\n",
    "# getting the dummies\n",
    "df_sea_cal_c = pd.concat([df_sea_cal_c.listing_id, pd.get_dummies(df_sea_cal_c.available, prefix='available')], axis=1)\n",
    "\n",
    "# grouping the amount of occupied and taken days for each listing\n",
    "seattle_calendar = df_sea_cal_c.groupby('listing_id').sum().reset_index()\n",
    "\n",
    "# creating the occupation_rate column: represents the % of the year that the listing is occupied\n",
    "seattle_calendar['occupation_rate'] = seattle_calendar['available_False'] / 365\n",
    "\n",
    "# removing unnecessary columns\n",
    "seattle_calendar = seattle_calendar[['listing_id', 'occupation_rate']]\n",
    "seattle_calendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 Combining Boston and Seattle Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calendar = pd.concat([boston_calendar, seattle_calendar])\n",
    "calendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Listings Dataset\n",
    "My objective with this dataset is to get the main features of each listing and it's review status. That way, I'll be able to run a model to predict the listing price and the review rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating copy of the dataset\n",
    "df_bos_lis_c = df_bos_lis.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Keeping only necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only necessary columns\n",
    "df_bos_lis_c = df_bos_lis_c[['id', 'market', 'host_is_superhost', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds', \n",
    "                             'bed_type', 'price', 'cleaning_fee', 'number_of_reviews', 'review_scores_rating', 'cancellation_policy']]\n",
    "\n",
    "df_bos_lis_c.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Checking data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking info of the dataset\n",
    "df_bos_lis_c.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the 'object' type columns to see if any of them should be a numerical one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling only the 'object' type column \n",
    "df_bos_lis_c.select_dtypes(include=['object']).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so there are 2 columns that should be numerical: 'price' and 'cleaning_fee'. **Let's fix them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the functions\n",
    "df_bos_lis_c.price = df_bos_lis_c.price.apply(lambda x: x.replace('$', ''))\n",
    "df_bos_lis_c.price = df_bos_lis_c.price.apply(lambda x: x.replace(',', '.'))\n",
    "df_bos_lis_c.price = df_bos_lis_c.price.apply(lambda x: x[:-3])\n",
    "\n",
    "df_bos_lis_c.cleaning_fee = df_bos_lis_c.cleaning_fee.apply(lambda x: str(x) if x else None)\n",
    "df_bos_lis_c.cleaning_fee = df_bos_lis_c.cleaning_fee.apply(lambda x: x.replace('$', '') if x else None)\n",
    "df_bos_lis_c.cleaning_fee = df_bos_lis_c.cleaning_fee.apply(lambda x: x.replace(',', '.') if x else None)\n",
    "df_bos_lis_c.cleaning_fee = df_bos_lis_c.cleaning_fee.apply(lambda x: x[:-3] if x else None)\n",
    "df_bos_lis_c.cleaning_fee = df_bos_lis_c.cleaning_fee.apply(lambda x: np.nan if x=='' else x)\n",
    "\n",
    "# checking columns\n",
    "df_bos_lis_c[['price', 'cleaning_fee']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK**, they look alright. Now I can fix the datatypes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the datatypes \n",
    "df_bos_lis_c[['price', 'cleaning_fee']] = df_bos_lis_c[['price', 'cleaning_fee']].astype('float64')\n",
    "df_bos_lis_c[['price', 'cleaning_fee']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Dealing with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking the amount of nulls in each column\n",
    "df_bos_lis_c.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling nulls in categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the amount of nulls in each object column  \n",
    "df_bos_lis_c.select_dtypes(include=['object']).isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How am I approaching this:\n",
    "* **Market**: all of them should be 'Boston' so I'll just put that \n",
    "* **Property_type**: Since it's only 3, I'll fill them with the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling market nulls\n",
    "df_bos_lis_c.market = df_bos_lis_c.market.fillna('Boston')\n",
    "\n",
    "# filling property_type nulls\n",
    "df_bos_lis_c.property_type = df_bos_lis_c.property_type.fillna(df_bos_lis_c.property_type.mode().values[0])\n",
    "\n",
    "# making sure there are no more nulls\n",
    "df_bos_lis_c.select_dtypes(include=['object']).isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling nulls in numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the amount of nulls in each numerical column\n",
    "df_bos_lis_c.select_dtypes(include=['int64', 'float64']).isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How am I approaching this:\n",
    "\n",
    "**bathrooms, bedrooms and beds**: Since there are only a handfull of these missing and calculating their mean doesn't make a lot of sense, I'll fill the null values with the mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling bathrooms null values with the mode\n",
    "df_bos_lis_c['bathrooms'] = df_bos_lis_c['bathrooms'].fillna(df_bos_lis_c['bathrooms'].mode().values[0])\n",
    "\n",
    "# filling bedrooms null values with the mode\n",
    "df_bos_lis_c['bedrooms'] = df_bos_lis_c['bedrooms'].fillna(df_bos_lis_c['bedrooms'].mode().values[0])\n",
    "\n",
    "# filling beds null values with the mode\n",
    "df_bos_lis_c['beds'] = df_bos_lis_c['beds'].fillna(df_bos_lis_c['beds'].mode().values[0])\n",
    "\n",
    "# making sure it worked\n",
    "df_bos_lis_c[['bathrooms', 'bedrooms', 'beds']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cleaning_fee**: By looking at the below chart it get's clear that it's better to fill the null values with the mean =~ 68. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_bos_lis_c['cleaning_fee'], bins=10)\n",
    "plt.title('cleaning_fee distribution', size=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling the null values with the mean \n",
    "df_bos_lis_c['cleaning_fee'] = df_bos_lis_c['cleaning_fee'].fillna(df_bos_lis_c['cleaning_fee'].mean())\n",
    "\n",
    "# checking if it worked\n",
    "df_bos_lis_c['cleaning_fee'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Dealing with Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bos_lis_c.select_dtypes(include='object').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**market**: I'll just create a 'boston' column that contains 1 for all values in boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates new column based on market column\n",
    "df_bos_lis_c['boston'] = np.where(df_bos_lis_c['market']=='Boston', 1, 0)\n",
    "df_bos_lis_c['seattle'] = np.where(df_bos_lis_c['market']=='Seattle', 1, 0)\n",
    "\n",
    "# drops olde market column\n",
    "df_bos_lis_c.drop('market', axis=1, inplace=True)\n",
    "\n",
    "# shows boston column\n",
    "df_bos_lis_c[['boston', 'seattle']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**host_is_superhost**: I'll again just create a new column 'superhost' that contains 1 if true else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates new column superhost based on host_is_superhost column\n",
    "df_bos_lis_c['superhost'] = np.where(df_bos_lis_c['host_is_superhost']=='t', 1, 0)\n",
    "\n",
    "# drop the 'host_is_superhost' column\n",
    "df_bos_lis_c.drop('host_is_superhost', axis=1, inplace=True)\n",
    "\n",
    "# shows superhost column\n",
    "df_bos_lis_c['superhost'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**property_type**: I'll get the dummies for this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bos_lis_c = pd.concat([df_bos_lis_c.drop('property_type',axis=1), pd.get_dummies(df_bos_lis_c.property_type)], axis=1)\n",
    "df_bos_lis_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**room_type**: I'll also just get the dummies for this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bos_lis_c = pd.concat([df_bos_lis_c.drop('room_type',axis=1), pd.get_dummies(df_bos_lis_c.room_type)], axis=1)\n",
    "df_bos_lis_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bed_type**: Again, just the dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bos_lis_c = pd.concat([df_bos_lis_c.drop('bed_type', axis=1), pd.get_dummies(df_bos_lis_c.bed_type)], axis=1)\n",
    "df_bos_lis_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cancellation_policy**: Just the dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bos_lis_c = pd.concat([df_bos_lis_c.drop('cancellation_policy', axis=1), pd.get_dummies(df_bos_lis_c.cancellation_policy)], axis=1)\n",
    "df_bos_lis_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rounding the 'cleaning_fee' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# applying the round function\n",
    "df_bos_lis_c['cleaning_fee'] = df_bos_lis_c['cleaning_fee'].round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the boston listings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boston_listings = df_bos_lis_c\n",
    "boston_listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 Applying the same steps to the Seattle dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating copy of the dataset\n",
    "df_sea_lis_c = df_sea_lis.copy()\n",
    "\n",
    "# selecting only necessary columns\n",
    "df_sea_lis_c = df_sea_lis_c[['id', 'market', 'host_is_superhost', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds', \n",
    "                             'bed_type', 'price', 'cleaning_fee', 'number_of_reviews', 'review_scores_rating', 'cancellation_policy']]\n",
    "\n",
    "# applying the functions\n",
    "df_sea_lis_c.price = df_sea_lis_c.price.apply(lambda x: x.replace('$', ''))\n",
    "df_sea_lis_c.price = df_sea_lis_c.price.apply(lambda x: x.replace(',', '.'))\n",
    "df_sea_lis_c.price = df_sea_lis_c.price.apply(lambda x: x[:-3])\n",
    "\n",
    "df_sea_lis_c.cleaning_fee = df_sea_lis_c.cleaning_fee.apply(lambda x: str(x) if x else None)\n",
    "df_sea_lis_c.cleaning_fee = df_sea_lis_c.cleaning_fee.apply(lambda x: x.replace('$', '') if x else None)\n",
    "df_sea_lis_c.cleaning_fee = df_sea_lis_c.cleaning_fee.apply(lambda x: x.replace(',', '.') if x else None)\n",
    "df_sea_lis_c.cleaning_fee = df_sea_lis_c.cleaning_fee.apply(lambda x: x[:-3] if x else None)\n",
    "df_sea_lis_c.cleaning_fee = df_sea_lis_c.cleaning_fee.apply(lambda x: np.nan if x=='' else x)\n",
    "\n",
    "# Changing the datatypes \n",
    "df_sea_lis_c[['price', 'cleaning_fee']] = df_sea_lis_c[['price', 'cleaning_fee']].astype('float64')\n",
    "\n",
    "# filling market nulls\n",
    "df_sea_lis_c.market = df_sea_lis_c.market.fillna('Boston')\n",
    "\n",
    "# filling property_type nulls\n",
    "df_sea_lis_c.property_type = df_sea_lis_c.property_type.fillna(df_sea_lis_c.property_type.mode().values[0])\n",
    "\n",
    "# filling bathrooms null values with the mode\n",
    "df_sea_lis_c['bathrooms'] = df_sea_lis_c['bathrooms'].fillna(df_sea_lis_c['bathrooms'].mode().values[0])\n",
    "\n",
    "# filling bedrooms null values with the mode\n",
    "df_sea_lis_c['bedrooms'] = df_sea_lis_c['bedrooms'].fillna(df_sea_lis_c['bedrooms'].mode().values[0])\n",
    "\n",
    "# filling beds null values with the mode\n",
    "df_sea_lis_c['beds'] = df_sea_lis_c['beds'].fillna(df_sea_lis_c['beds'].mode().values[0])\n",
    "\n",
    "# filling the null values with the mean \n",
    "df_sea_lis_c['cleaning_fee'] = df_sea_lis_c['cleaning_fee'].fillna(df_sea_lis_c['cleaning_fee'].mean())\n",
    "\n",
    "# creates new column based on market column\n",
    "df_sea_lis_c['boston'] = np.where(df_sea_lis_c['market']=='Boston', 1, 0)\n",
    "df_sea_lis_c['seattle'] = np.where(df_sea_lis_c['market']=='Seattle', 1, 0)\n",
    "\n",
    "# drops olde market column\n",
    "df_sea_lis_c.drop('market', axis=1, inplace=True)\n",
    "\n",
    "# creates new column superhost based on host_is_superhost column\n",
    "df_sea_lis_c['superhost'] = np.where(df_sea_lis_c['host_is_superhost']=='t', 1, 0)\n",
    "\n",
    "# drop the 'host_is_superhost' column\n",
    "df_sea_lis_c.drop('host_is_superhost', axis=1, inplace=True)\n",
    "\n",
    "# getting the dummies\n",
    "df_sea_lis_c = pd.concat([df_sea_lis_c.drop('property_type',axis=1), pd.get_dummies(df_sea_lis_c.property_type)], axis=1)\n",
    "df_sea_lis_c = pd.concat([df_sea_lis_c.drop('room_type',axis=1), pd.get_dummies(df_sea_lis_c.room_type)], axis=1)\n",
    "df_sea_lis_c = pd.concat([df_sea_lis_c.drop('bed_type', axis=1), pd.get_dummies(df_sea_lis_c.bed_type)], axis=1)\n",
    "df_sea_lis_c = pd.concat([df_sea_lis_c.drop('cancellation_policy', axis=1), pd.get_dummies(df_sea_lis_c.cancellation_policy)], axis=1)\n",
    "\n",
    "# applying the round function\n",
    "df_sea_lis_c['cleaning_fee'] = df_sea_lis_c['cleaning_fee'].round(decimals = 2)\n",
    "\n",
    "# creating the seattle listings dataset\n",
    "seattle_listings = df_sea_lis_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 Conbining seattle and boston datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(seattle_listings.head(1), boston_listings.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = pd.concat([seattle_listings, boston_listings])\n",
    "listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. As you can see, there were a few columns in the boston dataset there weren't in the seattle one. As consequence, thoso columns are completely NaN in the final dataset. Thankfully those are all binary columns, so I can just fill the up with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.7 Fixing the Listings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings[['Bungalow', 'Cabin', 'Chalet', 'Tent', 'Treehouse', 'Yurt', 'Entire Floor', \n",
    "          'Guesthouse', 'Villa', 'super_strict_30']] = listings[['Bungalow', 'Cabin', 'Chalet', 'Tent', 'Treehouse', \n",
    "                                                                 'Yurt', 'Entire Floor', 'Guesthouse', 'Villa', 'super_strict_30']].fillna(0)\n",
    "\n",
    "listings.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.8 Fixing the review_scores_rating column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings[(listings.review_scores_rating.isnull() == True) & (listings.number_of_reviews != 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, there are 2 different ocasions where the review_scores_rating is NaN\n",
    "1. The number of reviews is 0\n",
    "    - In this case, It makes sense to keep the rows and create a new column telling the the value is null\n",
    "2. The number of reviews is greater then 0\n",
    "    - In this case it doesn't make sense, so I'll just drop these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows with reviews and without review score\n",
    "listings = listings.drop(listings[(listings['review_scores_rating'].isnull()) & (listings['number_of_reviews']>0)].index)\n",
    "\n",
    "# checking if worked\n",
    "listings[(listings['review_scores_rating'].isnull()) & (listings['number_of_reviews']>0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a column to showcase were the reviews are missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings['missing_review'] = np.where(listings['review_scores_rating'].isnull(), 1, 0)\n",
    "listings[['missing_review', 'review_scores_rating']][listings['missing_review']==1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling review_scores_rating missing values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings['review_scores_rating'] = listings['review_scores_rating'].fillna(0)\n",
    "listings.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perfect!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Combining all three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_rev = pd.merge(calendar, reviews, left_on='listing_id', right_on='listing_id', how='left')\n",
    "cal_rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.merge(cal_rev, listings, left_on='listing_id', right_on='id', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have a few problems here. Again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Cleaning the final dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**problem 1**: There are a few (150) listings that weren't in the listings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['id'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My solution**: I'll need to drop these rows since it will be impossible to fill the these null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df['id'].isnull()].index)\n",
    "df[df['id'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**problem 2:** All the 1374 listings that doesn't have any reviews have null is the number of positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['review_negative'].isnull()) & (df['review_positive'].isnull()) & (df['review_unknwon'].isnull())].iloc[:, [2,3,4,12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**my solution:** I'll just add 0 for all those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, new there aren't any null values in the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Fixing the final df datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of columns that should be binary and are as float64 dataypes. I'll just change there datatypes to uint64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the first columns datatypes\n",
    "df.iloc[:, [2, 3, 4, 5, 6, 9, 12]] = df.iloc[:, [2, 3, 4, 5, 6, 9, 12]].astype('uint64')\n",
    "\n",
    "# changing the dataype of the rest of the columns\n",
    "df.iloc[:, 14:] = df.iloc[:, 14:].astype('uint64')\n",
    "\n",
    "# \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Dropping the unnecessary columns\n",
    "\n",
    "In this case, the listing_id and the id columns because they won't be useful for the modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['listing_id', 'id'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That looks great!** Now I can go for the modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Analysing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate my analysis, I'll need to undo some of the changes I previously did. For that, I'll just create a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new analysis df\n",
    "analysis_df = df.copy() \n",
    "\n",
    "# making a few changes to facilitate the analysis \n",
    "\n",
    "# creating categorical city column\n",
    "analysis_df['city'] = np.where(analysis_df.boston==1, 'boston', 'seattle')\n",
    "\n",
    "\n",
    "# creating categorical occupation column\n",
    "def classify_occupation(value: float) -> str:\n",
    "    \"\"\"Function to classify the occupation rate into categories, such as: \n",
    "    - always occupied\n",
    "    - often occupied \n",
    "    - sometimes occupied \n",
    "    - rarely occupied\n",
    "\n",
    "    Args:\n",
    "        value (float): the occupation rate of the propery\n",
    "\n",
    "    Returns:\n",
    "        str: the category in which the property fits\n",
    "    \"\"\"\n",
    "    if value >= 0.95:\n",
    "        return 'always occupied'\n",
    "    if value >= 0.75:\n",
    "        return 'often occupied'\n",
    "    if value >= 0.50:\n",
    "        return 'sometimes occupied'\n",
    "    if value >= 0.15:\n",
    "        return 'rarely occupied'\n",
    "    return 'almost never occupied'\n",
    "\n",
    "\n",
    "\n",
    "# creating categorical listing quality function\n",
    "def classify_listing_quality(value: float) -> str:\n",
    "    \"\"\"Function to classify the listing quality into categories, such as:\n",
    "    - amazing\n",
    "    - very good\n",
    "    - decent \n",
    "    - bad\n",
    "\n",
    "    Args:\n",
    "        value (float): Reviews rating for that listing\n",
    "\n",
    "    Returns:\n",
    "        str: category in which the property fits\n",
    "    \"\"\"\n",
    "    if value >= 0.75:\n",
    "        return 'good'\n",
    "    return 'bad'\n",
    "\n",
    "\n",
    "# applying functions to the dataset\n",
    "analysis_df['occupation'] = analysis_df['occupation_rate'].apply(classify_occupation)\n",
    "analysis_df['quality'] = analysis_df['review_scores_rating'].apply(classify_listing_quality)\n",
    "\n",
    "# removing outliers\n",
    "analysis_df = analysis_df.drop(analysis_df[analysis_df['occupation_rate']>1].index)\n",
    "\n",
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Question 1: \n",
    "\n",
    "#### **Which city has the best listings? Which one has more expensive ones? Is there a connection in that?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping data\n",
    "df1 = analysis_df.groupby('city')['review_scores_rating'].mean().to_frame().reset_index()\n",
    "\n",
    "# creating figure\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "# creating bar plot\n",
    "ax = sns.barplot(x='city', y='review_scores_rating', data=df1, palette=\"Set1\", edgecolor='black')\n",
    "\n",
    "# personalizing bar plot\n",
    "ax.bar_label(ax.containers[0], size=12);\n",
    "ax.set_title('Average Review Rating by City', size=15)\n",
    "ax.set_ylabel('Average Review Rating', size=12)\n",
    "ax.set_xlabel('City', size=12)\n",
    "\n",
    "# showing bar plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping data\n",
    "df2 = analysis_df.groupby('city')[['review_negative', 'review_positive']].sum()\n",
    "\n",
    "# building plot\n",
    "ax = df2.plot(kind='bar', stacked=False, color=['tomato', 'skyblue'], figsize=(12,8), edgecolor='black')\n",
    "\n",
    "# personalizing plot\n",
    "ax.set_title('Number of Positive and Negative Reviews by City', size=15)\n",
    "ax.set_xlabel('City', size=15)\n",
    "ax.set_ylabel('Number of Reviews', size=15)\n",
    "\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i, size=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping data\n",
    "df3 = analysis_df.groupby('city')['price'].mean().to_frame().reset_index()\n",
    "df3['price'] = df3['price'].apply(lambda x: round(x, 2))\n",
    "\n",
    "# creating figure\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "# building plot\n",
    "ax = sns.barplot(x='city', y='price', data=df3, palette='Set1', edgecolor='black')\n",
    "\n",
    "# personalizing plot\n",
    "ax.set_title('Average price by City', size=15)\n",
    "ax.set_ylabel('Average Price', size=15)\n",
    "ax.set_xlabel('City', size=15)\n",
    "ax.bar_label(ax.containers[0], size=12)\n",
    "\n",
    "# showing plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Results: \n",
    "- After looking at the data, it got clear that the city of **Seattle has the best listings**. It not only has a **higher average review rating**, 78.8 compared with 72.1 in Boston, but it also has a much higher positive/negative review ratio, 55.4 compared with 31.9 in Boston. In addition to that, **Boston has a much higher average price**, 168.70 dollars compared with 127.02 dollars in Seattle. That means that more expensive houses are not necessarily better. \n",
    "- In my opinion, when people get into more expensive houses they tend to be more demanding. In that case, sometimes it leads to people geting disapointed with the listing, leading to negative reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Question 2:\n",
    "\n",
    "### What's the connection between price and occupation rate? Where does the quality of the listing fits in that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# grouping data\n",
    "df4 = analysis_df.groupby('occupation')['price'].mean().to_frame().reset_index().sort_values(by='price')\n",
    "\n",
    "# plotting \n",
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.barplot(data=df4, x='occupation', y='price', edgecolor='black', palette='Set1')\n",
    "ax.set_xlabel('Occupation Rate', size=15)\n",
    "ax.set_ylabel('Average Price', size=15)\n",
    "ax.set_title('Average Price by Occupation Rate', size=15)\n",
    "\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i, size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the relationshiop between price and occupation_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.scatterplot(data=analysis_df, y='price', x='occupation_rate', hue='quality', palette='Set1')\n",
    "ax.set_xlabel('Occupation Rate', size=15)\n",
    "ax.set_ylabel('Price', size=15)\n",
    "ax.set_title('Price x Occupation Rate', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Results:\n",
    "- The correlation between price and occupation rate is not very strong. That means that lowering the price of your house won't necessarily call for more attention.\n",
    "- One interesting finding is that expensive houses (above 400 dollars) tend to have either a very high or very low occupation rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Question 3:\n",
    "\n",
    "### What are the main features that influences the review rates? What about the prices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer that question, I'll need to build some machine learning models. That leads us to the next section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Predicting the review scores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better predict the review scores, I'll first drop the 0 filled review_scores rating rows. That way our model won't be biased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows\n",
    "df_review = df.drop(df[df['review_scores_rating']==0].index)\n",
    "\n",
    "# checking dataframe\n",
    "df_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting into train and test datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressors\n",
    "X = df_review.drop(columns=['review_scores_rating', 'missing_review'], axis=1)\n",
    "\n",
    "# target\n",
    "y = df_review['review_scores_rating']\n",
    "\n",
    "# train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to automate model creation, training, and evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(ml_model, X_train, X_test, y_train, y_test, evaluation_methods: list):\n",
    "    \"\"\"Function to automate the model creation, training, prediction, and evaluation.\n",
    "\n",
    "    Args:\n",
    "        ml_model (object): Algorithm that will be built\n",
    "        X_train (Array): training regressors\n",
    "        X_test (Array): testing regressors\n",
    "        y_train (Array): training target\n",
    "        y_test (Array): testing target\n",
    "        evaluation_methods (list): evaluation methods that will be used to see the model performace\n",
    "    \"\"\"\n",
    "    # creates model\n",
    "    model = ml_model\n",
    "    \n",
    "    # trains model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # predicts model\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # evaluates model\n",
    "    evaluation_results = []\n",
    "    \n",
    "    for method in evaluation_methods:\n",
    "        evaluation_results.append(method(y_test, y_pred))\n",
    "        \n",
    "    # prints results\n",
    "    for method, result in zip(evaluation_methods, evaluation_results):\n",
    "        print(f'{method}: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Linear Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(LinearRegression(normalize=True), X_train, X_test, y_train, y_test, [r2_score, mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Ridge Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "build_model(RidgeCV(), X_train, X_test, y_train, y_test, [r2_score, mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Lasso Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "build_model(LassoCV(), X_train, X_test, y_train, y_test, [r2_score, mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Elastic Net Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "build_model(ElasticNetCV(), X_train, X_test, y_train, y_test, [r2_score, mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(RandomForestRegressor(), X_train, X_test, y_train, y_test, [r2_score, mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since the Random Forest was the best model, let's build it again and tune the hyperparameters:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before hyperparameter tuning:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# building model \n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# training model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predicting with the model\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# evaluating the model\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'R2 Score: {r2},\\nMean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After hyperparameter tuning:**\n",
    "- Using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# searching for parameters\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# train model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look on the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the model\n",
    "tuned_rf = rf_random.best_estimator_\n",
    "\n",
    "# training the model\n",
    "tuned_rf.fit(X_train, y_train)\n",
    "\n",
    "# predicting with the model\n",
    "y_pred_tuned = tuned_rf.predict(X_test)\n",
    "\n",
    "# evaluating the model\n",
    "r2 = r2_score(y_test, y_pred_tuned)\n",
    "mse = mean_squared_error(y_test, y_pred_tuned)\n",
    "\n",
    "print(f'R2 Score: {r2},\\nMean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that model isn't much better, but it has improved. Let's check the features importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(tuned_rf.feature_importances_, index = X_train.columns,columns=['importance']).sort_values('importance',ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Results:\n",
    "\n",
    "- The most important features when predicting an Airbnb listing review score is the price followed by the occupation rate and the number of good and bad reviews. **That actually makes a lot of sense**. \n",
    "    - People will evaluate your house based on the price you put on it. If you charge too much, people will probably be disappointed and review it poorly. \n",
    "- Other important indicator is the **cleaning fee**.\n",
    "    - Apperently people dont' like to spend a lot when it's time to clean. Quite interesting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Predicting the Price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressors\n",
    "X = df.drop(['price'], axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the models again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(LinearRegression(), X_train, X_test, y_train, y_test, [r2_score, mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(LassoCV(), X_train, X_test, y_train, y_test, [r2_score, mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(LassoCV(), X_train, X_test, y_train, y_test, [r2_score, mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ElasticNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(ElasticNetCV(), X_train, X_test, y_train, y_test, [r2_score, mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the previously created function\n",
    "build_model(RandomForestRegressor(), X_train, X_test, y_train, y_test, [r2_score, mean_squared_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the Random Forest Model was the best. Let's build it again and tune it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model \n",
    "rf_price = RandomForestRegressor()\n",
    "\n",
    "# training model\n",
    "rf_price.fit(X_train, y_train)\n",
    "\n",
    "# predicting with the model\n",
    "y_pred = rf_price.predict(X_test)\n",
    "\n",
    "# evaluating the model\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'R2 Score: {r2},\\nMean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "rf_price_tuned = RandomForestRegressor()\n",
    "\n",
    "# searching for parameters\n",
    "rf_price_tuned = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# train model\n",
    "rf_price_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the model\n",
    "tuned_rf = rf_random.best_estimator_\n",
    "\n",
    "# training the model\n",
    "tuned_rf.fit(X_train, y_train)\n",
    "\n",
    "# predicting with the model\n",
    "y_pred_tuned = tuned_rf.predict(X_test)\n",
    "\n",
    "# evaluating the model\n",
    "r2 = r2_score(y_test, y_pred_tuned)\n",
    "mse = mean_squared_error(y_test, y_pred_tuned)\n",
    "\n",
    "print(f'R2 Score: {r2},\\nMean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(tuned_rf.feature_importances_, index = X_train.columns,columns=['importance']).sort_values('importance',ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Results:\n",
    "- The most important features when predicting the price of a listing are:\n",
    "    1. The **cleaning fee**: When it's very expensive to clean a house, the house must be very expensive as well\n",
    "    2. The **number of bedrooms**: More bedrooms -> bigger house -> more expensive house. Makes Sense.\n",
    "    3. The **number of people it accommodates**: More people -> more bedrooms -> bigger house -> more expensive house. Makes Sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Results & Evaluation\n",
    "\n",
    "## Which city has the best listings? Which one has more expensive ones? Is there a conection in that?\n",
    "\n",
    "- After looking at the data, it got clear that the city of **Seattle has the best listings**. It not only has a **higher average review rating**, 78.8 compared with 72.1 in Boston, but it also has a much higher positive/negative review ratio, 55.4 compared with 31.9 in Boston. In addition to that, **Boston has a much higher average price**, 168.70 dollars compared with 127.02 dollars in Seattle. That means that more expensive houses are not necessarily better. \n",
    "- In my opinion, when people get into more expensive houses they tend to be more demanding. In that case, sometimes it leads to people geting disapointed with the listing, leading to negative reviews. \n",
    "\n",
    "## What's the connection between price and occupation rate? Where does the quality of the listing fits in that?\n",
    "\n",
    "- The correlation between price and occupation rate is not very strong. That means that lowering the price of your house won't necessarily call for more attention.\n",
    "- One interesting finding is that expensive houses (above 400 dollars) tend to have either a very high or very low occupation rate. \n",
    "\n",
    "## What are the main features that influence the review rates? What about the prices?\n",
    "\n",
    "### Review Rates:\n",
    "- The most important features when predicting an Airbnb listing review score is the price followed by the occupation rate and the number of good and bad reviews. **That actually makes a lot of sense**. \n",
    "    - People will evaluate your house based on the price you put on it. If you charge too much, people will probably be disappointed and review it poorly. \n",
    "- Other important indicator is the **cleaning fee**.\n",
    "    - Apperently people dont' like to spend a lot when it's time to clean. Quite interesting. \n",
    "    \n",
    "### Listing Prices:\n",
    "- The most important features when predicting the price of a listing are:\n",
    "    1. The **cleaning fee**: When it's very expensive to clean a house, the house must be very expensive as well\n",
    "    2. The **number of bedrooms**: More bedrooms -> bigger house -> more expensive house. Makes Sense.\n",
    "    3. The **number of people it accommodates**: More people -> more bedrooms -> bigger house -> more expensive house. Makes Sense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a9f8fe2dbe13acf2a3933ecae393e09c8689df079fdc02936947523c3be48f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
